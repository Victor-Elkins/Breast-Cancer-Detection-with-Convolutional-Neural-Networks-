{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub6f2XSfvr_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7442c3c4-92fa-4d30-cba1-697eeffe9e75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "files = os.listdir('/content/drive/MyDrive')\n",
        "print(files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-1ykGguwvHC",
        "outputId": "2957a529-f3f3-4663-a70d-cc6c96d25650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Untitled Diagram (1).drawio.png', 'Untitled Diagram.drawio.png', 'DS2 Notes.gdoc', 'Victor Elkins Resume.pdf', 'Colab Notebooks', 'Untitled project.gscript', 'CIA 1 Readthrough (1).mp4', 'CamelsAndHorses', 'archive', 'Untitled document (1).gdoc', 'CatDog', 'catvsdog.zip', 'Flowers Recognition Dataset.zip', 'Flowers Recognition Dataset', 'R2 Interview Presentation Guidelines  Agenda.pdf', 'CIA 1 Readthrough.mp4', 'catimage.jpg', 'yolov3.h5', 'yolov3.weights.1', 'yolov3.weights', 'gecko.jpg', 'tree.jpg', 'bird.jpg', 'yolov3.h5.1', 'yolov3.weights.2', 'yolov3.cfg', 'yolov3.h5.2', 'yolov3.weights.3', 'yolov3.h5.3', 'yolov3.weights.4', 'cifar-10-python.tar.gz', 'reddit.csv', 'Untitled presentation.gslides', '2024-10-18 15-58-30.mkv', '2024-10-18 16-28-08.mkv', 'R2 Interview Presentation Guidelines  Agenda.gdoc', 'images.jpg', 'Low quality dancing Cat meme.mp4', 'breastcancer', 'Untitled document.gdoc', 'monthly-milk-production.csv', 'archive(2)', 'photos', 'imagedata', 'CBR Learnings.gdoc', 'PIP Group Project', 'thurapy.gdoc', 'RAIHC Final Report.gdoc', 'carfolder', 'Project Presentation.gslides', 'Datasets Doc.gdoc', 'RAIHC Final presentation.gslides', 'extracted_data.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to the file\n",
        "file_path = '/content/drive/MyDrive/extracted_data.csv'\n",
        "\n",
        "# Load the file into a Pandas DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRsBpfHUwPEG",
        "outputId": "9a79abc1-86d0-474b-c8ca-c479aef2770a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   CoronaryAngina  Age_Group  Sex  Income_Category  Heavy_Alcohol_Consumption  \\\n",
            "0               1         12    0                6                          0   \n",
            "1               1         11    0                5                          0   \n",
            "2               1          8    0                5                          0   \n",
            "3               1         12    1                4                          0   \n",
            "4               1         12    0                4                          0   \n",
            "\n",
            "   BMI_Category  Exercise_Participation  Smoking_Status  Insurance_Type  Race  \n",
            "0             0                       1               0               0     2  \n",
            "1             2                       1               0               0     7  \n",
            "2             1                       1               0               4     2  \n",
            "3             0                       0               0               0     7  \n",
            "4             1                       0               0               0     2  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "\n",
        "weighted_percentages = {\n",
        "    'CoronaryAngina': [21.23, 78.77],  # [Yes, No]\n",
        "    'Age_Group': [12.37, 7.38, 9.11, 7.62, 8.47, 6.45, 7.83, 7.11, 8.60, 6.94, 6.48, 4.55, 5.24, 1.84],\n",
        "    'Sex': [48.50, 51.50],  # [Male, Female]\n",
        "    'Income_Category': [4.88, 7.53, 8.88, 10.42, 22.29, 17.49, 6.78, 21.73],\n",
        "    'Heavy_Alcohol_Consumption': [85.67, 5.19, 9.14],\n",
        "    'BMI_Category': [1.96, 30.50, 34.73, 32.80],  # [Underweight, Normal, Overweight, Obese]\n",
        "    'Exercise_Participation': [75.03, 24.71, 0.19, 0.07],\n",
        "    '_SmokingStatus': [83.08, 10.68, 6.25],  # [No, Yes, Unknown]\n",
        "    'Insurance_Type': [37.92, 8.99, 21.23, 0.08, 8.18, 0.04, 3.30, 0.15, 3.15, 3.27, 8.11, 3.82, 1.76]\n",
        "}\n",
        "\n",
        "def create_synthetic_data(n_samples=10000, weighted=False):\n",
        "    data = {}\n",
        "\n",
        "    if weighted:\n",
        "        for feature, weights in weighted_percentages.items():\n",
        "            if feature!= 'CoronaryAngina':\n",
        "                data[feature] = np.random.choice(\n",
        "                    len(weights),\n",
        "                    size=n_samples,\n",
        "                    p=np.array(weights) / np.sum(weights)\n",
        "                )\n",
        "    else:\n",
        "        features = ['Age_Group', 'Sex', 'Income_Category', 'Heavy_Alcohol_Consumption',\n",
        "                    'BMI_Category', 'Exercise_Participation', '_SmokingStatus', 'Insurance_Type']\n",
        "        for feature in features:\n",
        "            num_categories = len(weighted_percentages[feature])\n",
        "            data[feature] = np.random.choice(num_categories, size=n_samples)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    probabilities = 0.2\n",
        "    probabilities += 0.1 * (df['Age_Group'] > 7)\n",
        "    probabilities += 0.1 * (df['BMI_Category'] == 3)\n",
        "    probabilities += 0.05 * (df['_SmokingStatus'] == 1)\n",
        "\n",
        "    df['CoronaryAngina'] = np.random.binomial(1, probabilities)\n",
        "\n",
        "    smoking_mapping = {\n",
        "        1: \"Yes\",\n",
        "        2: \"Yes\",\n",
        "        3: \"No\",\n",
        "        4: \"No\"\n",
        "    }\n",
        "    df['_SmokingStatus'] = df['_SmokingStatus'].map(smoking_mapping)\n",
        "    df = df.dropna(subset=['_SmokingStatus'])\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_model(input_dim):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df_processed = df.copy()\n",
        "    protected_attribute = df_processed['Income_Category'].astype(float)\n",
        "\n",
        "    categorical_columns = ['Age_Group', 'Sex', 'Income_Category', 'Heavy_Alcohol_Consumption',\n",
        "                           'BMI_Category', 'Exercise_Participation', '_SmokingStatus', 'Insurance_Type']\n",
        "\n",
        "    df_encoded = pd.get_dummies(df_processed, columns=categorical_columns)\n",
        "\n",
        "    return df_encoded, protected_attribute\n",
        "\n",
        "def train_and_evaluate(weighted=False):\n",
        "    df = create_synthetic_data(weighted=weighted)\n",
        "    df_encoded, protected_attribute = preprocess_data(df)\n",
        "\n",
        "    X = df_encoded.drop('CoronaryAngina', axis=1)\n",
        "    y = df_encoded['CoronaryAngina'].astype(float)\n",
        "\n",
        "    X_train, X_test, y_train, y_test, protected_train, protected_test = train_test_split(\n",
        "        X, y, protected_attribute, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    X_train_tensor = tf.cast(X_train_scaled, tf.float32)\n",
        "    y_train_tensor = tf.cast(y_train, tf.float32)\n",
        "    protected_train_tensor = tf.cast(protected_train, tf.float32)\n",
        "\n",
        "    model = create_model(X_train.shape[1])\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=10, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_tensor,\n",
        "        y_train_tensor,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    y_pred = (model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, y_pred)\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1_score)\n",
        "    print(\"Support:\", support)\n",
        "\n",
        "    return precision, recall, f1_score, support\n",
        "\n",
        "def compare_results():\n",
        "    weighted_report = train_and_evaluate(weighted=True)\n",
        "    non_weighted_report = train_and_evaluate(weighted=False)\n",
        "\n",
        "    # Create a DataFrame from the reports\n",
        "    table = pd.DataFrame({\n",
        "        'Metric': ['Precision', 'Recall', 'F1-score', 'Support'],\n",
        "        'Weighted': list(weighted_report),\n",
        "        'Non-Weighted': list(non_weighted_report)\n",
        "    })\n",
        "\n",
        "    print(table)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    compare_results()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzH-1xoQwYXD",
        "outputId": "64133e5b-5104-4a4e-9fc9-7fc2b7d818f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "Classification Report:\n",
            "Precision: [0.75655431 0.40540541]\n",
            "Recall: [0.82113821 0.31578947]\n",
            "F1-score: [0.78752437 0.35502959]\n",
            "Support: [246  95]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
            "\n",
            "Classification Report:\n",
            "Precision: [0.70150502 0.39310345]\n",
            "Recall: [0.90507012 0.13768116]\n",
            "F1-score: [0.79039096 0.2039356 ]\n",
            "Support: [927 414]\n",
            "      Metric                                   Weighted  \\\n",
            "0  Precision  [0.7565543071161048, 0.40540540540540543]   \n",
            "1     Recall   [0.8211382113821138, 0.3157894736842105]   \n",
            "2   F1-score  [0.7875243664717348, 0.35502958579881655]   \n",
            "3    Support                                  [246, 95]   \n",
            "\n",
            "                                Non-Weighted  \n",
            "0   [0.7015050167224081, 0.3931034482758621]  \n",
            "1  [0.9050701186623517, 0.13768115942028986]  \n",
            "2    [0.790390956194065, 0.2039355992844365]  \n",
            "3                                 [927, 414]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "weighted_percentages = {\n",
        "    'CoronaryAngina': [21.23, 78.77],  # [Yes, No]\n",
        "    'Age_Group': [12.37, 7.38, 9.11, 7.62, 8.47, 6.45, 7.83, 7.11, 8.60, 6.94, 6.48, 4.55, 5.24, 1.84],\n",
        "    'Sex': [48.50, 51.50],  # [Male, Female]\n",
        "    'Income_Category': [4.88, 7.53, 8.88, 10.42, 22.29, 17.49, 6.78, 21.73],\n",
        "    'Heavy_Alcohol_Consumption': [85.67, 5.19, 9.14],\n",
        "    'BMI_Category': [1.96, 30.50, 34.73, 32.80],  # [Underweight, Normal, Overweight, Obese]\n",
        "    'Exercise_Participation': [75.03, 24.71, 0.19, 0.07],\n",
        "    '_SmokingStatus': [83.08, 10.68, 6.25],  # [No, Yes, Unknown]\n",
        "    'Insurance_Type': [37.92, 8.99, 21.23, 0.08, 8.18, 0.04, 3.30, 0.15, 3.15, 3.27, 8.11, 3.82, 1.76]\n",
        "}\n",
        "\n",
        "\n",
        "def create_synthetic_data(n_samples=10000):\n",
        "    data = {}\n",
        "\n",
        "    # Generate data for protected groups\n",
        "    women = np.random.choice([0, 1], size=n_samples, p=[0.5, 0.5])\n",
        "    african_americans = np.random.choice([0, 1], size=n_samples, p=[0.5, 0.5])\n",
        "    low_income = np.random.choice([0, 1], size=n_samples, p=[0.5, 0.5])\n",
        "\n",
        "    # Generate data for other features\n",
        "    age = np.random.choice(len(weighted_percentages['Age_Group']), size=n_samples, p=np.array(weighted_percentages['Age_Group']) / np.sum(weighted_percentages['Age_Group']))\n",
        "    bmi = np.random.choice(len(weighted_percentages['BMI_Category']), size=n_samples, p=np.array(weighted_percentages['BMI_Category']) / np.sum(weighted_percentages['BMI_Category']))\n",
        "    smoking = np.random.choice(len(weighted_percentages['_SmokingStatus']), size=n_samples, p=np.array(weighted_percentages['_SmokingStatus']) / np.sum(weighted_percentages['_SmokingStatus']))\n",
        "\n",
        "    # Generate target variable\n",
        "    coronary_angina = np.random.choice(len(weighted_percentages['CoronaryAngina']), size=n_samples, p=np.array(weighted_percentages['CoronaryAngina']) / np.sum(weighted_percentages['CoronaryAngina']))\n",
        "\n",
        "    # Create dataframe\n",
        "    df = pd.DataFrame({\n",
        "        'Women': women,\n",
        "        'African Americans': african_americans,\n",
        "        'Low Income': low_income,\n",
        "        'Age': age,\n",
        "        'BMI': bmi,\n",
        "        'Smoking': smoking,\n",
        "        'Coronary Angina': coronary_angina\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_model(input_dim):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_and_evaluate(df):\n",
        "    X = df.drop('Coronary Angina', axis=1)\n",
        "    y = df['Coronary Angina']\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(X_train.shape[1])\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "    # Calculate bias metrics\n",
        "    women_pred = y_pred[X_test['Women'] == 1]\n",
        "    african_americans_pred = y_pred[X_test['African Americans'] == 1]\n",
        "    low_income_pred = y_pred[X_test['Low Income'] == 1]\n",
        "\n",
        "    men_pred = y_pred[X_test['Women'] == 0]\n",
        "    non_african_americans_pred = y_pred[X_test['African Americans'] == 0]\n",
        "    high_income_pred = y_pred[X_test['Low Income'] == 0]\n",
        "\n",
        "    print(\"\\nBias Metrics:\")\n",
        "    print(\"Women:\", np.mean(women_pred))\n",
        "    print(\"African Americans:\", np.mean(african_americans_pred))\n",
        "    print(\"Low Income:\", np.mean(low_income_pred))\n",
        "    print(\"Men:\", np.mean(men_pred))\n",
        "    print(\"Non-African Americans:\", np.mean(non_african_americans_pred))\n",
        "    print(\"High Income:\", np.mean(high_income_pred))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = create_synthetic_data()\n",
        "    train_and_evaluate(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQLf1AFoxQw-",
        "outputId": "b9c5e2e6-6391-431b-8aae-866a13f9f01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       452\n",
            "           1       0.77      1.00      0.87      1548\n",
            "\n",
            "    accuracy                           0.77      2000\n",
            "   macro avg       0.39      0.50      0.44      2000\n",
            "weighted avg       0.60      0.77      0.68      2000\n",
            "\n",
            "\n",
            "Bias Metrics:\n",
            "Women: 1.0\n",
            "African Americans: 1.0\n",
            "Low Income: 1.0\n",
            "Men: 1.0\n",
            "Non-African Americans: 1.0\n",
            "High Income: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def create_synthetic_data(n_samples=10000):\n",
        "    data = {}\n",
        "\n",
        "    for feature, weights in weighted_percentages.items():\n",
        "        if feature != 'CoronaryAngina':\n",
        "            data[feature] = np.random.choice(\n",
        "                len(weights),\n",
        "                size=n_samples,\n",
        "                p=np.array(weights) / np.sum(weights)\n",
        "            )\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    probabilities = 0.2\n",
        "    probabilities += 0.1 * (df['Age_Group'] > 7)\n",
        "    probabilities += 0.1 * (df['BMI_Category'] == 3)\n",
        "    probabilities += 0.05 * (df['_SmokingStatus'] == 1)\n",
        "\n",
        "    df['CoronaryAngina'] = np.random.binomial(1, probabilities)\n",
        "\n",
        "    smoking_mapping = {\n",
        "        1: \"Yes\",\n",
        "        2: \"Yes\",\n",
        "        3: \"No\",\n",
        "        4: \"No\"\n",
        "    }\n",
        "    df['_SmokingStatus'] = df['_SmokingStatus'].map(smoking_mapping)\n",
        "    df = df.dropna(subset=['_SmokingStatus'])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_model(input_dim):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df_processed = df.copy()\n",
        "    # Assuming you want to use 'Income_Category' as the protected attribute before encoding\n",
        "    protected_attribute = df_processed['Income_Category'].astype(float)\n",
        "\n",
        "    categorical_columns = ['Age_Group', 'Sex', 'Income_Category', 'Heavy_Alcohol_Consumption',\n",
        "                           'BMI_Category', 'Exercise_Participation', '_SmokingStatus', 'Insurance_Type']\n",
        "\n",
        "    # Encode categorical columns\n",
        "    df_encoded = pd.get_dummies(df_processed, columns=categorical_columns)\n",
        "\n",
        "    return df_encoded, protected_attribute\n",
        "\n",
        "\n",
        "def train_and_evaluate():\n",
        "    df = create_synthetic_data()\n",
        "    df_encoded, protected_attribute = preprocess_data(df)\n",
        "\n",
        "    X = df_encoded.drop('CoronaryAngina', axis=1)\n",
        "    y = df_encoded['CoronaryAngina'].astype(float)\n",
        "\n",
        "    # Now use the protected_attribute, which is the original 'Income_Category'\n",
        "    X_train, X_test, y_train, y_test, protected_train, protected_test = train_test_split(\n",
        "        X, y, protected_attribute, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    X_train_tensor = tf.cast(X_train_scaled, tf.float32)\n",
        "    y_train_tensor = tf.cast(y_train, tf.float32)\n",
        "    protected_train_tensor = tf.cast(protected_train, tf.float32)\n",
        "\n",
        "    model = create_model(X_train.shape[1])\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=10, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Change verbose to 0 to suppress epoch-wise accuracy printing\n",
        "    history = model.fit(\n",
        "        X_train_tensor,\n",
        "        y_train_tensor,\n",
        "        epochs=1000,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=0,  # Set to 0 to suppress training progress\n",
        "    )\n",
        "\n",
        "    y_pred = (model.predict(X_test_scaled) > 0.5).astype(int)\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "    # Print bias metrics\n",
        "    print_bias_metrics(y_test, y_pred, protected_test)\n",
        "\n",
        "    # Plot results\n",
        "    plot_results(history)\n",
        "\n",
        "    return model, history, scaler\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, history, scaler = train_and_evaluate()\n"
      ],
      "metadata": {
        "id": "JS49d6QE5T2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hmg1mkhT6a3C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}